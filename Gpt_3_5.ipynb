{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAV749RH-vYC"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIWdYkU5-vYC"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "import re\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import string\n",
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo11haKl-vYD"
      },
      "source": [
        "# Data pre-processing:\n",
        "\n",
        "As the task of interest is the generate quality data from the book, we need to split the book into meaningful and coherent parts with the consideration of max token limitation of the model.\n",
        "*   For achieving this first I extracted chapter titles together with numbers for subsections.\n",
        "\n",
        "*  Then text of subsections were extracted.\n",
        "\n",
        "* If text of subsections were too long, (more tokens than the upperbound) it is splitted from a point in which there is a title within a subsection. This way, the document coherency won't be interrupted and generated data will be meaningful and good quality.\n",
        "\n",
        "* However, if the text is still above the token limit, it is simply splitted in half.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT9V9npb-vYD"
      },
      "outputs": [],
      "source": [
        "# For getting the name of chapters and subsections from the table of contents\n",
        "\n",
        "def extract_toc(pdf_path, toc_page_numbers):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        toc_data = {}\n",
        "        current_title = None\n",
        "        for page_number in toc_page_numbers:\n",
        "            page = reader.pages[page_number - 1]\n",
        "            page_text = page.extract_text()\n",
        "\n",
        "            lines = page_text.strip().split('\\n')\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "\n",
        "\n",
        "                subtitle_match = re.match(r\"^(\\d+\\.\\d+)\", line)\n",
        "                if subtitle_match and current_title is not None:\n",
        "                    subtitle_number = subtitle_match.group(1)\n",
        "                    toc_data[current_title].append(subtitle_number)\n",
        "                else:\n",
        "\n",
        "                    title_match = re.match(r\"^\\d [^\\.]+\", line)\n",
        "                    if title_match:\n",
        "                        title = line.split(' ', maxsplit=1)[1]\n",
        "                        toc_data[title] = []\n",
        "                        current_title = title\n",
        "\n",
        "    return toc_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rosgei-u-vYD",
        "outputId": "f2aabb8c-3390-4dd6-8427-803512a58372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Introduction 9': ['1.1', '1.2', '1.3'], 'Guessing Game 21': ['2.1', '2.2', '2.3', '2.4', '2.5', '2.6'], 'Common Programming Concepts 45': ['3.1', '3.2', '3.3', '3.4', '3.5'], 'Understanding Ownership 77': ['4.1', '4.2', '4.3'], 'Using Structs to Structure Related Data 105': ['5.1', '5.2', '5.3'], 'Enums and Pattern Matching 123': ['6.1', '6.2', '6.3'], 'Using Modules to Reuse and Organize Code 145': ['1.1', '1.2', '1.3'], 'Common Collections 169': ['2.1', '2.2', '2.3'], 'Error Handling 191': ['3.1', '3.2', '3.3'], 'Generic Types, T raits, and Lifetimes 213': ['4.1', '4.2', '4.3', '4.4'], 'T esting 255': ['5.1', '5.2', '5.3'], 'An I/O Project Building a Small Grep 287': ['6.1', '6.2', '6.3', '6.4', '6.5', '6.6'], 'F unctional Language features in Rust: Iterators and': ['1.1', '1.2', '1.3', '1.4'], 'More about Cargo and Crates.io 369': ['2.1', '2.2', '2.3', '2.4', '2.5'], 'Smart Pointers 385': ['3.1', '3.2', '3.3', '3.4', '3.5', '3.6'], 'F earless Concurrency 419': ['4.1', '4.2', '4.3', '4.4'], 'Is Rust an Object-Oriented Programming Language? 449': ['5.1', '5.2', '5.3'], 'Patterns Match the Structure of V alues 483': ['1.1', '1.2', '1.3'], 'Advanced F eatures 507': ['2.1', '2.2', '2.3', '2.4', '2.5'], 'Final Project: Building a Multithreaded W eb Server 551': ['3.1', '3.2', '3.3', '3.4', '3.5', '3.6']}\n"
          ]
        }
      ],
      "source": [
        "toc_titles = extract_toc(pdf_path, toc_page_numbers)\n",
        "print(toc_titles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-AKNMYx-vYE"
      },
      "outputs": [],
      "source": [
        "#Getting the text of subsections\n",
        "def extract_chapter_texts(pdf_path):\n",
        "    toc_page_numbers = [5, 6, 7, 8]\n",
        "    toc_titles_with_numbers = extract_toc(pdf_path, toc_page_numbers)\n",
        "\n",
        "    toc_list = [{\"title\": key, \"subtitles\": value} for key, value in toc_titles_with_numbers.items()]\n",
        "\n",
        "    chapter_texts = []\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(reader.pages)\n",
        "\n",
        "        for toc_data in toc_list:\n",
        "            chapter_title = toc_data[\"title\"]\n",
        "            subtitle_numbers = toc_data[\"subtitles\"]\n",
        "\n",
        "            chapter_start_page_match = re.search(r'\\d+', chapter_title)\n",
        "            chapter_start_page = int(chapter_start_page_match.group()) if chapter_start_page_match else 1\n",
        "\n",
        "            chapter_end_page_match = re.search(r'\\d+', toc_list[toc_list.index(toc_data) + 1][\"title\"]) if toc_list.index(toc_data) < len(toc_list) - 1 else None\n",
        "            chapter_end_page = int(chapter_end_page_match.group()) + 1 if chapter_end_page_match else num_pages - 1\n",
        "\n",
        "            chapter_text = \"\"\n",
        "            for page_number in range(chapter_start_page, chapter_end_page):\n",
        "                page = reader.pages[page_number - 1]\n",
        "                page_text = page.extract_text()\n",
        "                chapter_text += page_text\n",
        "\n",
        "            cleaned_chapter_text = chapter_text.replace('\\n', ' ').strip()\n",
        "\n",
        "            if chapter_title not in toc_titles_with_numbers.keys():\n",
        "                chapter_dict = {'chapter_name': chapter_title, 'chapter_text': cleaned_chapter_text}\n",
        "                chapter_texts.append(chapter_dict)\n",
        "\n",
        "            for subtitle_number in subtitle_numbers:\n",
        "                subtitle_start = cleaned_chapter_text.find(subtitle_number)\n",
        "                subtitle_end = (\n",
        "                    cleaned_chapter_text.find(subtitle_numbers[subtitle_numbers.index(subtitle_number) + 1])\n",
        "                    if subtitle_numbers.index(subtitle_number) < len(subtitle_numbers) - 1\n",
        "                    else len(cleaned_chapter_text)\n",
        "                )\n",
        "                subtitle_text = cleaned_chapter_text[subtitle_start:subtitle_end].strip()\n",
        "\n",
        "                subtitle_key = f\"{chapter_title} {subtitle_number}\"\n",
        "                if subtitle_key not in toc_titles_with_numbers.keys():\n",
        "                    subtitle_dict = {\n",
        "                        'chapter_name': subtitle_key,\n",
        "                        'chapter_text': subtitle_text\n",
        "                    }\n",
        "                    chapter_texts.append(subtitle_dict)\n",
        "\n",
        "    return chapter_texts\n",
        "\n",
        "\n",
        "def remove_dicts_with_empty_text(chapters_list):\n",
        "    return [chapter_dict for chapter_dict in chapters_list if any(c.isalnum() for c in chapter_dict['chapter_text'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mji0tdvP-vYE"
      },
      "outputs": [],
      "source": [
        "#Splitting the subsections from a point where there is a title if they exceed token limit (3000 tokens roughly)\n",
        "#This way is better than random splitting as document coherence will be preserved which is highly important for getting quality outputs from the model\n",
        "def tokenize_chapter_text(chapters_list):\n",
        "    new_chapters_list = []\n",
        "    for chapter_dict in chapters_list:\n",
        "        chapter_title = chapter_dict['chapter_name']\n",
        "        chapter_text = chapter_dict['chapter_text']\n",
        "        tokens = nltk.word_tokenize(chapter_text)\n",
        "        if len(tokens) > 3000:\n",
        "            lines = chapter_text.split('.')\n",
        "            split_point = None\n",
        "            for i in range(len(lines)):\n",
        "                line = lines[i]\n",
        "                if re.match(r'^([A-Z][^.!?]*[^.!?] ?)+$', line):\n",
        "                    split_point = i\n",
        "                    break\n",
        "            if split_point:\n",
        "\n",
        "                first_part_text = '\\n'.join(lines[:split_point])\n",
        "                second_part_title = lines[split_point]\n",
        "                second_part_dict = {\n",
        "                    'chapter_name': second_part_title,\n",
        "                    'chapter_text': '\\n'.join(lines[split_point + 1:])\n",
        "                }\n",
        "\n",
        "\n",
        "                first_part_dict = {\n",
        "                    'chapter_name': chapter_title,\n",
        "                    'chapter_text': first_part_text\n",
        "            }\n",
        "                new_chapters_list.append(first_part_dict)\n",
        "                new_chapters_list.append(second_part_dict)\n",
        "\n",
        "            else:\n",
        "                new_chapters_list.append(chapter_dict)\n",
        "\n",
        "        else:\n",
        "            new_chapters_list.append(chapter_dict)\n",
        "\n",
        "    return new_chapters_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdCh5mc8-vYE"
      },
      "outputs": [],
      "source": [
        "def split_text_into_halves(chapters):\n",
        "    def split_text(text):\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        mid_index = len(sentences) // 2\n",
        "        first_half = \" \".join(sentences[:mid_index])\n",
        "        second_half = \" \".join(sentences[mid_index:])\n",
        "        return first_half, second_half\n",
        "\n",
        "    result = []\n",
        "    for d in chapters:\n",
        "          first_half, second_half = split_text(d[1])\n",
        "          result.append((d[0],first_half))\n",
        "          result.append((d[0],second_half))\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nck8V7bQ-vYE"
      },
      "outputs": [],
      "source": [
        "pdf_path = './data/rust-doc.pdf'\n",
        "chapters = extract_chapter_texts(pdf_path)\n",
        "del chapters[0] #as it is authors note\n",
        "chapters=remove_dicts_with_empty_text(chapters) #deleting the summary parts``\n",
        "chapters=tokenize_chapter_text(chapters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhmz2vC1-vYE",
        "outputId": "6e4b7dd9-58d5-4fe4-a80f-308f341e1ce3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "execution_count": 750,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chapters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuFMIFYd-vYE"
      },
      "source": [
        "# TASK\n",
        "#### The task of interest is to build a coherent simple synthetic dataset for finetuning an code generator LLM. So, we need data as pairs of problem/instruction and code solution.\n",
        "\n",
        "- After playing with different ways to prompt the model, I find stating explicitly the nature of the problem and answer as most efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APYF133Q-vYF"
      },
      "outputs": [],
      "source": [
        "def get_response(document):\n",
        "    task = (\n",
        "    f\"Read the following document about Rust Programming Language and create a 2 Q&A problems where the question is a coding problem and the answer is the code. {document}\\n\"\n",
        ")\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[\n",
        "        {'role': 'user', 'content': task}\n",
        "    ],\n",
        "        temperature=1,\n",
        "        max_tokens=700\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5KnSSZ4-vYF"
      },
      "source": [
        "### Toy example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_XISjnS-vYF"
      },
      "outputs": [],
      "source": [
        "example=chapters[57][\"chapter_name\"] + \"\\n\" + chapters[]['chapter_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbkZL69M-vYF"
      },
      "outputs": [],
      "source": [
        "exp_response=get_response(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gyCOmrv-vYF",
        "outputId": "fdccbb19-6875-4ff8-9987-d45c11c5b73f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Q1: Write code to create a channel and send an integer value of 10 down the channel.**\n",
            "\n",
            "```rust\n",
            "use std::sync::mpsc;\n",
            "\n",
            "fn main() {\n",
            "    let (tx, rx) = mpsc::channel();\n",
            "    tx.send(10).unwrap();\n",
            "}\n",
            "```\n",
            "\n",
            "**Q2: Write code to create a channel and receive a string value from the channel, then print it out.**\n",
            "\n",
            "```rust\n",
            "use std::sync::mpsc;\n",
            "\n",
            "fn main() {\n",
            "    let (tx, rx) = mpsc::channel();\n",
            "    tx.send(String::from(\"Hello\")).unwrap();\n",
            "\n",
            "    let received = rx.recv().unwrap();\n",
            "    println!(\"Received: {}\", received);\n",
            "}\n",
            "```\n",
            "\n",
            "**Q3: Write code to create a channel and spawn two threads that send string values \"hello\" and \"world\" to the channel, then receive these values from the channel and print them out.**\n",
            "\n",
            "```rust\n",
            "use std::thread;\n",
            "use std::sync::mpsc;\n",
            "\n",
            "fn main() {\n",
            "    let (tx, rx) = mpsc::channel();\n",
            "\n",
            "    let tx1 = tx.clone();\n",
            "    thread::spawn(move || {\n",
            "        tx.send(String::from(\"hello\")).unwrap();\n",
            "    });\n",
            "\n",
            "    thread::spawn(move || {\n",
            "        tx1.send(String::from(\"world\")).unwrap();\n",
            "    });\n",
            "\n",
            "    let received1 = rx.recv().unwrap();\n",
            "    let received2 = rx.recv().unwrap();\n",
            "\n",
            "    println!(\"Received: {}, {}\", received1, received2);\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(exp_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md0HSGjj-vYF"
      },
      "source": [
        "## Enitire docs\n",
        "#### - My approach is to first try to get outputs for meaningful chunks of documents (either sub-chappters of subsections of chapters) to ensure quality of our syntetic data.\n",
        "#### - The documents that we can't input due to token limit will be splitted into two halves with the split_text_into_halves function  and then will be given as an input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbQyqJ3k-vYF"
      },
      "outputs": [],
      "source": [
        "def generate_qa(chapters_list, max_tokens=3200):\n",
        "    responses = []\n",
        "    skipped_documents = []\n",
        "\n",
        "    for idx, chapter_dict in enumerate(chapters_list):\n",
        "        chapter_name = chapter_dict['chapter_name']\n",
        "        chapter_text = chapter_dict['chapter_text']\n",
        "        document = f\"{chapter_name}\\n{chapter_text}\"\n",
        "\n",
        "        if len(nltk.word_tokenize(document)) <= max_tokens:\n",
        "            response = get_response(document)\n",
        "            responses.append((idx, response))\n",
        "        else:\n",
        "            skipped_documents.append((chapter_name, chapter_text))\n",
        "\n",
        "    return responses, skipped_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGOQ9Q1q-vYF"
      },
      "outputs": [],
      "source": [
        "responses, skipped_docs= generate_qa(chapters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi5510RP-vYF",
        "outputId": "bca1fc2f-1b28-4e5d-d935-cad2dc081fe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0,\n",
              " 'Q1: How do you install Rust on Linux or Mac?\\nA1: Open a terminal and type the following command: $ curl https://sh.rustup.rs -sSf | sh\\n\\nQ2: How do you update Rust to the latest version?\\nA2: From your shell, run the update script: $ rustup update')"
            ]
          },
          "execution_count": 751,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "responses[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhYJldhT-vYF"
      },
      "outputs": [],
      "source": [
        "output = [list(item) for item in responses]\n",
        "file_path = \"/Users/gamzegoren/Desktop/output.json\"\n",
        "\n",
        "with open(file_path, \"w\") as json_file:\n",
        "    json.dump(output, json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFYkklce-vYF",
        "outputId": "551bbef9-d090-46a1-f901-5e82e7b23ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: How do you install Rust on Linux or Mac?\n",
            "A1: Open a terminal and type the following command: $ curl https://sh.rustup.rs -sSf | sh\n"
          ]
        }
      ],
      "source": [
        "sequence=responses[0][1]\n",
        "lines = sequence.split('\\n')\n",
        "question1 = None\n",
        "answer1 = None\n",
        "\n",
        "for line in lines:\n",
        "    if line.startswith(\"Q1:\"):\n",
        "        question1 = line.replace(\"Q1:\", \"\").strip()\n",
        "    elif line.startswith(\"A1:\"):\n",
        "        answer1 = line.replace(\"A1:\", \"\").strip()\n",
        "\n",
        "print(\"Q1:\", question1)\n",
        "print(\"A1:\", answer1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMyfSHNs-vYG"
      },
      "source": [
        "### For docs exceeded token limit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn25jmGO-vYG"
      },
      "outputs": [],
      "source": [
        "split_docs=split_text_into_halves(skipped_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-w32s1R-vYG",
        "outputId": "4630c57e-318e-4ee3-c60d-0d74ffeb3876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "execution_count": 831,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(skipped_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCoVcjhO-vYG",
        "outputId": "e02ab29a-a88e-44a7-ae22-8e1e6d642ecb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "execution_count": 828,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "063YVIqP-vYG"
      },
      "outputs": [],
      "source": [
        "outputs = []\n",
        "for doc in split_docs:\n",
        "    try:\n",
        "        output = get_response(doc[1])\n",
        "        outputs.append((doc[0], output))\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred for document {doc[0]}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2GzvFTG-vYH",
        "outputId": "19acaa4f-96e6-4df9-a709-a77f9239159b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Understanding Ownership 77 4.1',\n",
              " 'Q1: Write a code to demonstrate ownership in Rust.\\nA1: \\n```\\nfn main() {\\n    let x = 5; // x owns the value 5\\n    let y = x; // y now owns the value 5, x is no longer valid\\n    println!(\"{}\", y); // Print the value of y\\n}\\n```\\n\\nQ2: Write a code to demonstrate dynamic memory allocation in Rust.\\nA2:\\n```\\nfn main() {\\n    let mut s = String::from(\"hello\"); // Allocate memory on the heap for the string \"hello\"\\n    s.push_str(\", world!\"); // Append \", world!\" to the string\\n    println!(\"{}\", s); // Print the value of s (\"hello, world!\")\\n}\\n```')"
            ]
          },
          "execution_count": 834,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0qSLmFw-vYH"
      },
      "outputs": [],
      "source": [
        "file_path = '/Users/gamzegoren/Desktop/response.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump (total_responses, json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NeoWQDB0-vYH"
      },
      "outputs": [],
      "source": [
        "file_path = '/Users/gamzegoren/Desktop/response.json'\n",
        "with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jnECwFz-vYH",
        "outputId": "af94b2ba-f957-4e1f-b7b3-6adbdc0c7a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 'Q1: How do you install Rust on Linux or Mac?\\nA1: Open a terminal and type the following command: $ curl https://sh.rustup.rs -sSf | sh\\n\\nQ2: How do you update Rust to the latest version?\\nA2: From your shell, run the update script: $ rustup update']\n"
          ]
        }
      ],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlM0ebWQ-vYH"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data, columns=['Section', 'Output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOMpBM_M-vYK",
        "outputId": "ee9fc103-bfc1-4e74-f91e-1b5f2b8e2531"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Section</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Q1: How do you install Rust on Linux or Mac?\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Q1. Write a Rust program that prints the numbe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Q1: Write a program in Rust that prints \"Hello...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Q1: How do you prompt the user to input their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Q1: Write a code to generate a random number b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Section                                             Output\n",
              "0       0  Q1: How do you install Rust on Linux or Mac?\\n...\n",
              "1       1  Q1. Write a Rust program that prints the numbe...\n",
              "2       2  Q1: Write a program in Rust that prints \"Hello...\n",
              "3       3  Q1: How do you prompt the user to input their ...\n",
              "4       4  Q1: Write a code to generate a random number b..."
            ]
          },
          "execution_count": 877,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2Dbw2Y7-vYL"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/Users/gamzegoren/Desktop/GPT_QAs.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gB-3F1LKGkd"
      },
      "source": [
        "# Possible Future Directions:\n",
        "- Since the task of generating synthetic dataset from a given document requires careful evaluation of the outputs, different evaluation methods should be applied to instructions, the answers and their combinations. This might include semantic similarity of the among pairs and to the document, as well as n-gram overlap of with the document etc.\n",
        "- Also, some hyperparameter tuning for obtaining better results should be considered."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZAV749RH-vYC",
        "Bo11haKl-vYD",
        "AuFMIFYd-vYE",
        "l5KnSSZ4-vYF",
        "Md0HSGjj-vYF",
        "zMyfSHNs-vYG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "dd94487997fdd450534b0b37167e4c5b379d7633462d20c26c6aae860a87e886"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
